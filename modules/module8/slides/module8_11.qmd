---
format: revealjs
title: Working with Null Values
title-slide-attributes:
  data-notes: |
---

```{python}
#  | echo: false
%run src/utils.py
```


**Null**: The human-readable term of a value that is missing from the dataframe.

```{python}
np.nan
```

Missing values are sometimes referred to as `NA` values.

In this course, we generally refer to them as both ***null*** and `NaN` values. 


:::{.notes} 
In the real world of data analysis, it's uncommon that we have a perfect dataset ready to be used. In fact, in most cases, cleaning and wrangling data will be an ongoing and time-consuming project. No matter how complete or well planned a database may seem, a data analyst will almost always encounter ***null*** values. 

A "null" is the human-readable term of a value that is missing from the dataframe. Remember, in Module 4 we discussed `NaN` being of type `float`? Python translates null values in numerical columns to `NaN`. Well, `NaN` is a constant that comes from the NumPy library. 

In some cases, missing values are sometimes referred to as `NA` values because of how they are handled in other programming languages. 

This is reflected in some of the names of the functions we use to handle them.   

In this course, we generally refer to them as both ***null*** and `NaN` values.
:::

---

## Info on missing values 

```{python}
# | include: false
cereal = pd.read_csv('data/cereal.csv').loc[:,["name", "mfr", "calories", "fat", "fiber", "rating"]]
```

```{python}
cereal.info()
```


:::{.notes} 
A good rule of thumb when conducting an analysis is to check early on how complete the dataset is. 

`.info()` is similar to `.dtypes`, but in addition to the dtype of each column, it includes the total number of non-null values contained in each column.

Let's try it out on a subset of our `cereal` dataset.

Here we see the total number of rows at the top with `RangeIndex: 77 entries, 0 to 76`. 

The `Non-Null Count` column specifies the number of non-null values. 

In this case, we have a complete dataframe with zero null values for each column.
:::

---

```{python}
# | include: false
cycling = pd.read_csv('data/cycling_data_dirty.csv', parse_dates =["Date"])
```

```{python}
cycling
```


:::{.notes} 
Let's take a look at a case where we are not so lucky. `cycling` is a subset of a dataset that contains the bicycling trips <a href="https://www.tomasbeuzen.com/" target="_blank">Tomas Beuzen</a>, a UBC postdoc rode his bike to campus and back during the fall 2019 semester.
:::

---

```{python}
cycling.info()
```


:::{.notes} 
Using `.info()` with this new data, we get the following.

We can see that there is a total of 33 entries (rows). 

We see that the `Distance` column only contains 30 non-null values out of a possible 33. 

This would mean that 3 values are missing from this column.
:::

---



```{python}
cycling['Distance'].isnull()
```

:::{.notes} 
We can use `.isnull()` on a particular column to obtain a Boolean series indicating if each row is a null value.

:::

---

```{python}
cycling[cycling['Distance'].isnull()]
```

<br>

```{python}
cycling[cycling.isnull().any(axis=1)]
```


:::{.notes} 
We can pair `.isnull()` with our filtering method to obtain the rows that contain null values in the `Distance` column of the dataframe.

Here, we see the 3 rows of our dataframe that contain null values. 

If we wanted to filter all the rows that contain null values and not just in the `Distance` column, we could use the verb `.any()` on the full dataframe.

We only have `NaN` values in the `Distance` column, so the same 3 rows are outputted as before.
:::

---

We will be discussing the following 2 simple ways of working with missing values:  

- <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html" target="_blank">`.dropna()`</a>
- <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html" target="_blank">`.fillna()`</a>


:::{.notes} 
Now that we have identified that our dataframe contains null values, what can we do about them?
:::

---

## Dropping Null Values 

```{python}
trips_removed = cycling.dropna()
trips_removed
```


:::{.notes} 
The easiest and simplest way of handling nulls values is to remove those rows from the dataset. 

In a fashion similar to dropping columns, we can drop rows if they contain a `NaN` value. 

It's important that we take some necessary precautions and not drop a large portion of the data. 

In our example above, if we were to remove the 3 rows we identified to contain `NaN` values, we do it in the following way.

Notice that index 2 was removed, and we only have 30 rows in our dataframe now.
:::

---

```{python}
cycling.dropna(subset=['Type'])
```


:::{.notes}
By default, all the rows with `NaN` values in any column will be considered when dropping rows; however, if we only want to drop rows with `NaN` values in certain columns, we can use the `subset` argument.

Since in this code, we subsetting the column `Type` which has no `NaN` values, no rows were dropped from the dataframe, and we still have 33 rows.
:::

---

```{python}
cycling.dropna(subset=['Distance'])
```


:::{.notes} 
The rows do get dropped when we subset on the `Distance` column. 

Alternatively, if you have a column missing a large portion of data, the best option may be to drop that column instead of the rows with missing values.

This will keep more of your data instead of dropping and losing most of your data.
:::

---

## Replacing Null Values

```{python}
cycling_zero_fill = cycling.fillna(value=0)
cycling_zero_fill
```


:::{.notes} 
Alternately, if we have a small dataset and we don't want to rid ourselves of any data, we may prefer to replace `NaN` with a particular value. 

We can do so with `.fillna()`.

Perhaps it's missing from the data because he didn't cycle that particular day. Replacing the `NaN` value with 0, in this case, would make sense.

Now index 2 now has a `Distance` of `0.00`.
:::

---

```{python}
cycling['Distance'].mean().round(2)
```

<br>

```{python}
cycling_mean_fill = cycling.fillna(value=cycling['Distance'].mean().round(2))
cycling_mean_fill
```


:::{.notes} 
Maybe a better decision would be to replace the values in `Distance` with the mean to avoid outliers. 

First, we can calculate the mean rounded to 2 decimal places as 12.67, and include that in the `value` argument in the `.fillna()` verb. 

We can now see the value in `Distance` for index 2 change to `12.67`.
:::

---

```{python}
cycling.fillna(method='bfill')
```


:::{.notes} 
We could also fill using certain methods.       

***"bfill"*** uses the next valid row observation to fill the `NaN`:

Index 2 adopts the distance value of `12.84` from index 3.
:::

---

```{python}
cycling.fillna(method='ffill')
```


:::{.notes} 
The `method` argument value ***"ffill"*** propagates the last valid observation forward to next.

Here, we see that index 2 adopts the value `13.03` from index 1. 

`bfill` and `ffill` are methods usually adopted when dealing with columns organized by date. 

This way, an observation can adopt a similar value to those near it. 

We will explore date columns in the next slide deck. 

Remember, these are only a few methods that can be used in simple situations.

In some scenarios, more complex methods of handling missing values may need to be used for effective analysis. 
:::


# Letâ€™s apply what we learned!